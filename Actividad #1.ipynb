{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOteB39IiX+SoE2Gis0UYh8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanEan/Juan/blob/main/Actividad%20%231.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Cargar y preparar los datos\n",
        "Data = pd.read_csv(\"/content/data_adults.csv\")\n",
        "\n",
        "# Eliminar columnas irrelevantes\n",
        "Data_cop = Data.drop([\"fnlwgt\", \"education-num\"], axis=1)\n",
        "\n",
        "# Separar características (X) y variable objetivo (y)\n",
        "X = Data_cop.drop(\"income\", axis=1)\n",
        "y = Data_cop['income'].isin(['>50K.', '>50K']).astype(int)\n",
        "\n",
        "# Identificar características numéricas y categóricas\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Transformador para características numéricas\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Transformador para características categóricas para SVM y Gradient Boosting\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Preprocesador general para SVM y Gradient Boosting\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Transformador para características categóricas específico para Naive Bayes\n",
        "categorical_transformer_nb = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Usar sparse_output=False\n",
        "])\n",
        "\n",
        "# Preprocesador específico para Naive Bayes\n",
        "preprocessor_nb = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer_nb, categorical_features)\n",
        "    ])\n",
        "\n",
        "# División de los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Usar una muestra más pequeña de los datos para pruebas rápidas\n",
        "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, test_size=0.8, random_state=42)\n",
        "\n",
        "# Función para mostrar los mejores parámetros y resultados\n",
        "def mostrar_resultados(random_search, model_name):\n",
        "    print(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
        "    print(f\"Best score for {model_name}: {random_search.best_score_}\")\n",
        "\n",
        "# Optimización de Hiperparámetros\n",
        "\n",
        "# 1. SVM\n",
        "svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('classifier', SVC())])\n",
        "\n",
        "param_grid_svm = {\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__gamma': [1, 0.1, 0.01],\n",
        "    'classifier__kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "random_search_svm = RandomizedSearchCV(svm_pipeline, param_distributions=param_grid_svm, n_iter=5, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search_svm.fit(X_train_sample, y_train_sample)\n",
        "mostrar_resultados(random_search_svm, \"SVM\")\n",
        "\n",
        "# 2. Gradient Boosting\n",
        "gb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('classifier', GradientBoostingClassifier())])\n",
        "\n",
        "param_grid_gb = {\n",
        "    'classifier__n_estimators': [50, 100, 150],\n",
        "    'classifier__learning_rate': [0.1, 0.05],\n",
        "    'classifier__max_depth': [3, 4]\n",
        "}\n",
        "\n",
        "random_search_gb = RandomizedSearchCV(gb_pipeline, param_distributions=param_grid_gb, n_iter=5, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search_gb.fit(X_train_sample, y_train_sample)\n",
        "mostrar_resultados(random_search_gb, \"Gradient Boosting\")\n",
        "\n",
        "# 3. Naive Bayes\n",
        "nb_pipeline = Pipeline(steps=[('preprocessor', preprocessor_nb),\n",
        "                              ('classifier', GaussianNB())])\n",
        "\n",
        "param_grid_nb = {\n",
        "    'classifier__var_smoothing': np.logspace(0, -9, num=10)  # Reducir rango de var_smoothing para rapidez\n",
        "}\n",
        "\n",
        "random_search_nb = RandomizedSearchCV(nb_pipeline, param_distributions=param_grid_nb, n_iter=5, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search_nb.fit(X_train_sample, y_train_sample)\n",
        "mostrar_resultados(random_search_nb, \"Naive Bayes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AuX__855vMt",
        "outputId": "33d23135-b6bb-4187-f7b8-8a7f53e58007"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for SVM: {'classifier__kernel': 'linear', 'classifier__gamma': 0.1, 'classifier__C': 1}\n",
            "Best score for SVM: 0.8446687143483985\n",
            "Best parameters for Gradient Boosting: {'classifier__n_estimators': 150, 'classifier__max_depth': 4, 'classifier__learning_rate': 0.1}\n",
            "Best score for Gradient Boosting: 0.8636829018575399\n",
            "Best parameters for Naive Bayes: {'classifier__var_smoothing': 1.0}\n",
            "Best score for Naive Bayes: 0.794793037882112\n"
          ]
        }
      ]
    }
  ]
}